{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpXLJZYu1_4j"
   },
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2ERHkbgy1ulT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjomfTdt18o5"
   },
   "source": [
    "# 2. Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dt9H5Nf2vJ-"
   },
   "source": [
    "## 2.1 Metric Monitor\n",
    "Keeps track of average over values added to its instance. Useful to track accuracy over batches and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnyphpqb10fG"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=4):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sp1tsqfS2z5Z"
   },
   "source": [
    "## 2.2 Early Stopping\n",
    "Early stopping is a form of regularization used to avoid overfitting on the training dataset. Early stopping keeps track of the validation loss, if the loss stops decreasing for several epochs in a row the training stops. The ```EarlyStopping``` class is used to create an object to keep track of the validation loss. It will save a checkpoint of the model each time the validation loss decrease.  We set the ```patience``` argument to how many epochs we want to wait after the last time the validation loss improved before breaking the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zh7tkIkK13dQ"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Source:\n",
    "            https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'> early stopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLFtDthc2R8Z"
   },
   "source": [
    "## 2.3 Calculate Accuracy\n",
    "Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZtARFoQ2hE_"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, target):\n",
    "    \"Calculates accuracy\"\n",
    "    output = output.data.max(dim=1,keepdim=True)[1]\n",
    "    output = output == 1.0\n",
    "    output = torch.flatten(output)\n",
    "    target = target == 1.0\n",
    "    target = torch.flatten(target)\n",
    "    return torch.true_divide((target == output).sum(dim=0), output.size(0)).item() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13lsLXFQ2CtG"
   },
   "source": [
    "# 3. Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrYrVHEv2L_d"
   },
   "source": [
    "## 3.1 Model\n",
    "A simple Convolutional Neural Network (CNN) for image classification. Input consists of grayscale images 28x28. Output consists of 10 neurons representing proabilities for the 10 classes of MNIST dataset. The specific CNN aims to demonstrate the use of every possible layer present in any CNN (convolutional layer, pooling layer, fully-connected layer), regularization techniques (batch normalization, dropout), activation functions (here: ReLU), and weight initialization (here: xavier initializtion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhCOJQS92eTr"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # L1 (?, 28, 28, 1) -> (?, 28, 28, 32) -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "            )\n",
    "        # L2 (?, 14, 14, 32) -> (?, 14, 14, 64) -> (?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "            )\n",
    "        # L3 (?, 7, 7, 64) -> (?, 7, 7, 128) -> (?, 4, 4, 128)\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "            )\n",
    "        # L4 FC 4 x 4 x 128 inputs -> 625 outputs\n",
    "        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "            )\n",
    "        # L5 Final FC 625 inputs -> 10 outputs\n",
    "        self.fc2 = torch.nn.Linear(625, 10, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten them for FC\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.nn.functional.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRtcShmV2WvX"
   },
   "source": [
    "## 3.2 Training\n",
    "Main training loop of **model** for a number of batches over an epoch, as it is defined in [PyTorch](https://pytorch.org/). If CUDA is availiable, training will take place in GPU. ```EarlyStopping``` class is used to keep track of loss and accuracy. Returns the loss and accuracy of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWJL-DyW2j5g"
   },
   "outputs": [],
   "source": [
    "def training(epoch, model, train_loader, optimizer, criterion):\n",
    "    \"Training over an epoch\"\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data,labels = data.cuda(), labels.cuda()\n",
    "        data , labels = torch.autograd.Variable(data,False), torch.autograd.Variable(labels)\n",
    "        output = model(data.float())\n",
    "        loss = criterion(output, labels) \n",
    "        accuracy = calculate_accuracy(output, labels)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        data.detach()\n",
    "        labels.detach()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"[Epoch: {epoch:03d}] Train      | {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n",
    "    return metric_monitor.metrics['Loss']['avg'], metric_monitor.metrics['Accuracy']['avg']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTEqEQpS2W_k"
   },
   "source": [
    "## 3.3 Validation\n",
    "Main validation loop of **model** for a number of batches over an epoch, as it is defined in [PyTorch](https://pytorch.org/). If CUDA is availiable, validation will take place in GPU. ```EarlyStopping``` class is used to keep track of loss and accuracy. Returns the loss and accuracy of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHCnAbSV2kj6"
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, valid_loader, criterion):\n",
    "    \"Validation over an epoch\"\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(valid_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                data,labels = data.cuda(), labels.cuda()\n",
    "            data, labels = torch.autograd.Variable(data,False), torch.autograd.Variable(labels)\n",
    "            output = model(data.float())\n",
    "            loss = criterion(output,labels) \n",
    "            accuracy = calculate_accuracy(output, labels)\n",
    "            metric_monitor.update(\"Loss\", loss.item())\n",
    "            metric_monitor.update(\"Accuracy\", accuracy)\n",
    "            data.detach()\n",
    "            labels.detach()\n",
    "    print(\"[Epoch: {epoch:03d}] Validation | {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n",
    "    return metric_monitor.metrics['Loss']['avg'], metric_monitor.metrics['Accuracy']['avg']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HUFJyf02aYV"
   },
   "source": [
    "# 4. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5XDfwlD2loD"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    num_epochs = 100\n",
    "    use_early_stopping = True\n",
    "    use_scheduler = True\n",
    "    \n",
    "    model = Model().cuda() if torch.cuda.is_available() else Model()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5,), (0.5,)),\n",
    "                                   ])\n",
    "    \n",
    "    train_set = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
    "    valid_set = datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=256, shuffle=True)\n",
    "    \n",
    "    train_losses , train_accuracies = [],[]\n",
    "    valid_losses , valid_accuracies = [],[]\n",
    "    \n",
    "    if use_early_stopping:\n",
    "        early_stopping = EarlyStopping(patience=30, verbose=False, delta=1e-4)\n",
    " \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "        train_loss, train_accuracy = training(epoch,model,train_loader,optimizer,criterion)\n",
    "        valid_loss, valid_accuracy = validation(epoch,model,valid_loader,criterion)\n",
    "        \n",
    "        if use_scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "             \n",
    "        if use_early_stopping: \n",
    "            early_stopping(valid_loss, model)\n",
    "            \n",
    "            if early_stopping.early_stop:\n",
    "                print('Early stopping at epoch', epoch)\n",
    "                #model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "                break\n",
    "     \n",
    "    plt.plot(range(1,len(train_losses)+1), train_losses, color='b', label = 'training loss')\n",
    "    plt.plot(range(1,len(valid_losses)+1), valid_losses, color='r', linestyle='dashed', label = 'validation loss')\n",
    "    plt.legend(), plt.ylabel('loss'), plt.xlabel('epochs'), plt.title('Loss'), plt.show()\n",
    "     \n",
    "    plt.plot(range(1,len(train_accuracies)+1),train_accuracies, color='b', label = 'training accuracy')\n",
    "    plt.plot(range(1,len(valid_accuracies)+1),valid_accuracies, color='r', linestyle='dashed', label = 'validation accuracy')\n",
    "    plt.legend(), plt.ylabel('loss'), plt.xlabel('epochs'), plt.title('Accuracy'), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTxkyIgK2n2M"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
