{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpXLJZYu1_4j"
   },
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ERHkbgy1ulT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjomfTdt18o5"
   },
   "source": [
    "# 2. Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dt9H5Nf2vJ-"
   },
   "source": [
    "## 2.1 Metric Monitor\n",
    "Keeps track of average over values added to its instance. Useful to track accuracy over batches and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnyphpqb10fG"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=4):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sp1tsqfS2z5Z"
   },
   "source": [
    "## 2.2 Early Stopping\n",
    "Early stopping is a form of regularization used to avoid overfitting on the training dataset. Early stopping keeps track of the validation loss, if the loss stops decreasing for several epochs in a row the training stops. The ```EarlyStopping``` class is used to create an object to keep track of the validation loss. It will save a checkpoint of the model each time the validation loss decrease.  We set the ```patience``` argument to how many epochs we want to wait after the last time the validation loss improved before breaking the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zh7tkIkK13dQ"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Source:\n",
    "            https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'> early stopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLtlGGP42-mk"
   },
   "source": [
    "## 2.3 Attention\n",
    "* Attention\n",
    "* Gated Attention\n",
    "* MIL pool (mean/max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aA92OLnc3AVr"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Attention, self).__init__()\n",
    "        self.name = 'Attention'\n",
    "        self.L = 500\n",
    "        self.D = 128\n",
    "        self.K = 1\n",
    "\n",
    "        self.feature_extractor = model\n",
    "        self._to_linear = model._to_linear\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self._to_linear, self.L),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L*self.K, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor(x)\n",
    "        H = H.view(-1, self._to_linear)\n",
    "        H = self.fc(H)  # [b x L]\n",
    "\n",
    "        A = self.attention(H)  # [b x K]\n",
    "        A = torch.transpose(A, 1, 0)  # [K x b]\n",
    "        A = F.softmax(A, dim=1)  # softmax over b\n",
    "            \n",
    "        M = torch.mm(A, H)  # [K x L]\n",
    "\n",
    "        Y_prob = self.classifier(M)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat\n",
    "        \n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().data\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, Y_hat = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        loss = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class GatedAttention(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(GatedAttention, self).__init__()\n",
    "        self.name = 'Gated Attention'\n",
    "        self.L = 500\n",
    "        self.D = 128\n",
    "        self.K = 1\n",
    "\n",
    "        self.feature_extractor = model\n",
    "        self._to_linear = model._to_linear\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self._to_linear, self.L),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh())\n",
    "\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        self.attention_weights = nn.Linear(self.D, self.K)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L*self.K, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor(x)\n",
    "        H = H.view(-1, self._to_linear)\n",
    "        H = self.fc(H)  # [b x L]\n",
    "\n",
    "        A_V = self.attention_V(H)  # [b x D]\n",
    "        A_U = self.attention_U(H)  # [b x D]\n",
    "        A = self.attention_weights(A_V * A_U) # element wise multiplication -> [b x K]\n",
    "        A = torch.transpose(A, 1, 0)  # [K x b]\n",
    "        A = F.softmax(A, dim=1)  # softmax over b\n",
    "\n",
    "        M = torch.mm(A, H)  # [K x L]\n",
    "\n",
    "        Y_prob = self.classifier(M)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat\n",
    "    \n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().data\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, _ = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        loss = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class MIL_pool(nn.Module):\n",
    "    def __init__(self, model, operator='mean'):\n",
    "        super(MIL_pool, self).__init__()\n",
    "        self.L = 500 \n",
    "        if operator == 'mean':\n",
    "            self.operator = 'mean'\n",
    "        elif operator == 'max':\n",
    "            self.operator = 'max'    \n",
    "        else:\n",
    "            raise NotImplementedError('Operator not supported: {}'.format(operator))\n",
    "\n",
    "        self.name = 'MIL pool ' + self.operator\n",
    "        self.feature_extractor = model\n",
    "        self._to_linear = model._to_linear\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self._to_linear, self.L),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "        \n",
    "        # prepNN\n",
    "        H = self.feature_extractor(x)\n",
    "        H = H.view(-1, self._to_linear)\n",
    "        H = self.fc(H)  # [b x L]\n",
    "        \n",
    "        # aggregate function\n",
    "        if self.operator == 'mean':\n",
    "            M = torch.mean(H, 0)\n",
    "        else:\n",
    "            M = torch.amax(H, 0)\n",
    "          \n",
    "        # afterNN\n",
    "        Y_prob = self.classifier(M)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat\n",
    "        \n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().data\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, Y_hat = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        loss = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_D9UqHsb3A1v"
   },
   "source": [
    "## 2.4 Bad Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i33GBI33C50"
   },
   "outputs": [],
   "source": [
    "class BagDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, loader, dataset_length, target_number=9, mean_bag_length=10, var_bag_length=2, num_bag=250, seed=1):\n",
    "        self.target_number = target_number\n",
    "        self.mean_bag_length = mean_bag_length\n",
    "        self.var_bag_length = var_bag_length\n",
    "        self.num_bag = num_bag\n",
    "        self.loader = loader\n",
    "        self.r = np.random.RandomState(seed)\n",
    "        self.dataset_length = dataset_length # 60.000 for train MNIST\n",
    "        self.loader = loader\n",
    "        \n",
    "        self.bag_list, self.labels_list = self._create_bags()\n",
    "\n",
    "    def _create_bags(self):\n",
    "        for (batch_data, batch_labels) in self.loader:\n",
    "            all_imgs = batch_data\n",
    "            all_labels = batch_labels\n",
    "\n",
    "        bags_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        for i in range(self.num_bag):\n",
    "            bag_length = np.int(self.r.normal(self.mean_bag_length, self.var_bag_length, 1))\n",
    "            if bag_length < 1:\n",
    "                bag_length = 1\n",
    "\n",
    "            indices = torch.LongTensor(self.r.randint(0, self.dataset_length, bag_length))\n",
    "\n",
    "            labels_in_bag = all_labels[indices]\n",
    "            labels_in_bag = labels_in_bag == self.target_number\n",
    "\n",
    "            bags_list.append(all_imgs[indices])\n",
    "            labels_list.append(labels_in_bag)\n",
    "\n",
    "        return bags_list, labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bag = self.bag_list[index]\n",
    "        label = [max(self.labels_list[index]), self.labels_list[index]]\n",
    "        return bag, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLFtDthc2R8Z"
   },
   "source": [
    "## 2.5 Calculate Accuracy\n",
    "Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZtARFoQ2hE_"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, target):\n",
    "    \"Calculates accuracy\"\n",
    "    output = output.data.max(dim=1,keepdim=True)[1]\n",
    "    output = output == 1.0\n",
    "    output = torch.flatten(output)\n",
    "    target = target == 1.0\n",
    "    target = torch.flatten(target)\n",
    "    return torch.true_divide((target == output).sum(dim=0), output.size(0)).item() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13lsLXFQ2CtG"
   },
   "source": [
    "# 3. Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrYrVHEv2L_d"
   },
   "source": [
    "## 3.1 Model\n",
    "A simple Convolutional Neural Network (CNN) for image classification. Input consists of grayscale images 28x28. The feature extractor is only present since attention mechanism and used for classification. The specific CNN aims to demonstrate the use of every possible layer present in any CNN (convolutional layer, pooling layer, fully-connected layer), regularization techniques (batch normalization, dropout), and activation functions (here: ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhCOJQS92eTr"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \"Convolutional Neural Network\"\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # L1 (?, 28, 28, 1) -> (?, 28, 28, 32) -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "            )\n",
    "        # L2 (?, 14, 14, 32) -> (?, 14, 14, 64) -> (?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "            )\n",
    "        # L3 (?, 7, 7, 64) -> (?, 7, 7, 128) -> (?, 4, 4, 128)\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "            )\n",
    "        self._to_linear = 4 * 4 * 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten them for FC\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTEqEQpS2W_k"
   },
   "source": [
    "## 3.2 Training\n",
    "Main training loop of **model** for a number of batches over an epoch, as it is defined in [PyTorch](https://pytorch.org/). If CUDA is availiable, training will take place in GPU. ```EarlyStopping``` class is used to keep track of loss and accuracy. Returns the loss and accuracy of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHCnAbSV2kj6"
   },
   "outputs": [],
   "source": [
    "def training(epoch, model, train_loader, optimizer, criterion):\n",
    "    \"Training over an epoch\"\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        bag_label = labels[0]\n",
    "        if torch.cuda.is_available():\n",
    "            data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data , bag_label = torch.autograd.Variable(data,False), torch.autograd.Variable(bag_label)\n",
    "        loss = model.calculate_objective(data.float(), bag_label)\n",
    "        error, _ = model.calculate_classification_error(data.float(), bag_label)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        metric_monitor.update(\"Accuracy\", 1-error)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"[Epoch: {epoch:03d}] Train      | {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n",
    "    return metric_monitor.metrics['Loss']['avg'], metric_monitor.metrics['Accuracy']['avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abiLjLhf3yQK"
   },
   "source": [
    "## 3.3 Validation\n",
    "Main validation loop of **model** for a number of batches over an epoch, as it is defined in [PyTorch](https://pytorch.org/). If CUDA is availiable, validation will take place in GPU. ```EarlyStopping``` class is used to keep track of loss and accuracy. Returns the loss and accuracy of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XF9NIV7T3zrR"
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, valid_loader, criterion):\n",
    "    \"Validation over an epoch\"\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    for batch_idx, (data, labels) in enumerate(valid_loader):\n",
    "        bag_label = labels[0]\n",
    "        if torch.cuda.is_available():\n",
    "            data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = torch.autograd.Variable(data,False), torch.autograd.Variable(bag_label)\n",
    "        loss = model.calculate_objective(data.float(), bag_label)\n",
    "        error, predicted_label = model.calculate_classification_error(data.float(), bag_label)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        metric_monitor.update(\"Accuracy\", 1-error)\n",
    "    print(\"[Epoch: {epoch:03d}] Validation | {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n",
    "    return metric_monitor.metrics['Loss']['avg'], metric_monitor.metrics['Accuracy']['avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HUFJyf02aYV"
   },
   "source": [
    "# 4. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5XDfwlD2loD"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    num_epochs = 100\n",
    "    use_early_stopping = True\n",
    "    use_scheduler = True\n",
    "    attention_type = 'mil_pool_max' # choose among attention, gated_attention, mil_pool_mean, mil_pool_max\n",
    "    \n",
    "    if attention_type == 'attention': \n",
    "        model = Attention(Model()).cuda() if torch.cuda.is_available() else Attention(Model())\n",
    "    elif attention_type == 'gated_attention':\n",
    "        model = GatedAttention(Model()).cuda() if torch.cuda.is_available() else GatedAttention(Model())\n",
    "    elif attention_type == 'mil_pool_mean':\n",
    "        model = MIL_pool(Model(), 'mean').cuda() if torch.cuda.is_available() else MIL_pool(Model(), 'mean')\n",
    "    elif attention_type == 'mil_pool_max':\n",
    "        model = MIL_pool(Model(), 'max').cuda() if torch.cuda.is_available() else MIL_pool(Model(), 'max')\n",
    "    else:\n",
    "        raise NotImplementedError('Attention mechanism is not implemented or does not exist')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5,), (0.5,)),\n",
    "                                   ])\n",
    "    \n",
    "    train_set = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
    "    valid_set = datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
    "    \n",
    "    num_train = len(train_set)\n",
    "    num_valid = len(valid_set)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=num_train, shuffle=False)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=num_valid, shuffle=False)\n",
    "        \n",
    "    train_loader_bags = torch.utils.data.DataLoader(BagDataset(\n",
    "                                                        loader=train_loader,\n",
    "                                                        dataset_length=num_train,\n",
    "                                                        target_number=9,\n",
    "                                                        mean_bag_length=10,\n",
    "                                                        var_bag_length=2,\n",
    "                                                        num_bag=100,\n",
    "                                                        seed=1,\n",
    "                                                        ),\n",
    "                                                    batch_size=1,\n",
    "                                                    shuffle=True)\n",
    "    valid_loader_bags = torch.utils.data.DataLoader(BagDataset(\n",
    "                                                        loader=valid_loader,\n",
    "                                                        dataset_length=num_valid,\n",
    "                                                        target_number=9,\n",
    "                                                        mean_bag_length=10,\n",
    "                                                        var_bag_length=2,\n",
    "                                                        num_bag=250,\n",
    "                                                        seed=1,\n",
    "                                                        ),\n",
    "                                                    batch_size=1,\n",
    "                                                    shuffle=False)\n",
    "    \n",
    "    train_losses , train_accuracies = [],[]\n",
    "    valid_losses , valid_accuracies = [],[]\n",
    "    \n",
    "    if use_early_stopping:\n",
    "        early_stopping = EarlyStopping(patience=30, verbose=False, delta=1e-4)\n",
    " \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "        train_loss, train_accuracy = training(epoch,model,train_loader_bags,optimizer,criterion)\n",
    "        valid_loss, valid_accuracy = validation(epoch,model,valid_loader_bags,criterion)\n",
    "        \n",
    "        if use_scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "             \n",
    "        if use_early_stopping: \n",
    "            early_stopping(valid_loss, model)\n",
    "            \n",
    "            if early_stopping.early_stop:\n",
    "                print('Early stopping at epoch', epoch)\n",
    "                #model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "                break\n",
    "     \n",
    "    plt.plot(range(1,len(train_losses)+1), train_losses, color='b', label = 'training loss')\n",
    "    plt.plot(range(1,len(valid_losses)+1), valid_losses, color='r', linestyle='dashed', label = 'validation loss')\n",
    "    plt.legend(), plt.ylabel('loss'), plt.xlabel('epochs'), plt.title('Loss'), plt.show()\n",
    "     \n",
    "    plt.plot(range(1,len(train_accuracies)+1),train_accuracies, color='b', label = 'training accuracy')\n",
    "    plt.plot(range(1,len(valid_accuracies)+1),valid_accuracies, color='r', linestyle='dashed', label = 'validation accuracy')\n",
    "    plt.legend(), plt.ylabel('loss'), plt.xlabel('epochs'), plt.title('Accuracy'), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTxkyIgK2n2M"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST_classification_main_mil.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
